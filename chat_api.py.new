import torch
import pickle
import numpy as np
from flask import Flask, request, jsonify, send_from_directory
import os
import time
import signal
from collections import defaultdict
from model import GPT, GPTConfig

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException()

class time_limit:
    def __init__(self, seconds):
        self.seconds = seconds
        
    def __enter__(self):
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(self.seconds)
        
    def __exit__(self, type, value, traceback):
        signal.alarm(0)

# Use absolute paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, 'out', 'model.pt')
VOCAB_PATH = os.path.join(BASE_DIR, 'data', 'void', 'vocab.pkl')
META_PATH = os.path.join(BASE_DIR, 'data', 'void', 'meta.pkl')

print("Loading vocab...")
with open(VOCAB_PATH, 'rb') as f:
    chars, stoi = pickle.load(f)
itos = {i: ch for ch, i in stoi.items()}
vocab_size = len(chars)

print("Loading meta...")
with open(META_PATH, 'rb') as f:
    meta = pickle.load(f)

print("Configuring model...")
config = GPTConfig(
    vocab_size=vocab_size,
    block_size=meta.get('block_size', 64),
    n_layer=meta.get('n_layer', 4),
    n_head=meta.get('n_head', 4),
    n_embd=meta.get('n_embd', 128),
    dropout=meta.get('dropout', 0.1),
    bias=meta.get('bias', True)
)

print("Creating model...")
model = GPT(config)
print("Loading weights...")
model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
model.eval()

app = Flask(__name__, static_folder='.', static_url_path='')

@app.route('/')
def serve_index():
    return send_from_directory('.', 'index.html')

@app.route('/<path:path>')
def serve_static(path):
    response = send_from_directory('.', path)
    if path.endswith(('.js', '.css', '.html')):
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
    return response

RATE_LIMIT = 5
RATE_WINDOW = 1
request_counts = defaultdict(list)

def is_rate_limited(ip):
    now = time.time()
    request_counts[ip] = [t for t in request_counts[ip] if t > now - RATE_WINDOW]
    request_counts[ip].append(now)
    return len(request_counts[ip]) > RATE_LIMIT

@app.before_request
def check_rate_limit():
    if request.endpoint == 'chat':
        ip = request.remote_addr
        if is_rate_limited(ip):
            return jsonify({'error': 'Too many requests. Please wait a moment.'}), 429

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        prompt = data.get('prompt', '')
        if not prompt:
            return jsonify({'response': 'No prompt provided.'})

        if len(prompt) > 5000:
            return jsonify({'error': 'Prompt too long. Maximum length is 5000 characters.'}), 400

        try:
            idx = torch.tensor([[stoi.get(c, 0) for c in prompt]], dtype=torch.long)
        except Exception as e:
            return jsonify({'error': f'Error encoding prompt: {str(e)}'}), 500

        try:
            with torch.no_grad():
                max_new_tokens = 100
                temperature = 0.8
                
                idx = idx[:, -config.block_size:]
                
                for _ in range(max_new_tokens):
                    logits, _ = model(idx)
                    logits = logits[:, -1, :] / temperature
                    probs = torch.nn.functional.softmax(logits, dim=-1)
                    idx_next = torch.multinomial(probs, num_samples=1)
                    idx = torch.cat((idx, idx_next), dim=1)
                    if itos[idx_next.item()] == '\n':
                        break

            generated_sequence = idx[0].tolist()
            full_response = ''.join([itos[i] for i in generated_sequence])
            generated_text = full_response[len(prompt):]
            
            if not generated_text.strip():
                return jsonify({'error': 'Model generated empty response'}), 500
                
            return jsonify({'response': generated_text.strip()})

        except Exception as e:
            return jsonify({'error': f'Error generating response: {str(e)}'}), 500

    except Exception as e:
        return jsonify({'error': f'Server error: {str(e)}'}), 500

def cleanup_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    import gc
    gc.collect()

@app.after_request
def after_request(response):
    cleanup_memory()
    return response

@app.after_request
def add_security_headers(response):
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    response.headers['Content-Security-Policy'] = "default-src 'self'; img-src 'self' data:; style-src 'self' 'unsafe-inline'"
    return response

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 10000)))
