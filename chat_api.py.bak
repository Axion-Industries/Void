import torch
import pickle
import numpy as np
from flask import Flask, request, jsonify, send_from_directory
import os
import time
import signal
from collections import defaultdict
from model import GPT, GPTConfig
import time
import signal
from collections import defaultdict
from model import GPT, GPTConfig

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException()

class time_limit:
    def __init__(self, seconds):
        self.seconds = seconds
        
    def __enter__(self):
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(self.seconds)
        
    def __exit__(self, type, value, traceback):
        signal.alarm(0)
import time
import signal
from collections import defaultdict

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException()

class time_limit:
    def __init__(self, seconds):
        self.seconds = seconds
        
    def __enter__(self):
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(self.seconds)
        
    def __exit__(self, type, value, traceback):
        signal.alarm(0)

# Use absolute paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, 'out', 'model.pt')
VOCAB_PATH = os.path.join(BASE_DIR, 'data', 'void', 'vocab.pkl')
META_PATH = os.path.join(BASE_DIR, 'data', 'void', 'meta.pkl')

# Check for required files before loading
missing_files = []
for f in [MODEL_PATH, VOCAB_PATH, META_PATH]:
    if not os.path.exists(f):
        missing_files.append(f)

app = Flask(__name__, static_folder='.', static_url_path='')

@app.route('/')
def serve_index():
    return send_from_directory('.', 'index.html')

@app.route('/<path:path>')
def serve_static(path):
    response = send_from_directory('.', path)
    if path.endswith(('.js', '.css', '.html')):
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
    return response

if missing_files:
    @app.route('/chat', methods=['POST'])
    def chat_error():
        return jsonify({'error': f"Missing required model files: {', '.join(missing_files)}. Please train your model and add these files to your repo."}), 500
    if __name__ == '__main__':
        print(f"Missing required model files: {', '.join(missing_files)}. Please train your model and add these files to your repo.")
        app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 10000)))
    exit(0)

# Load model and vocab
print(f"Loading vocab from {VOCAB_PATH}")
with open(VOCAB_PATH, 'rb') as f:
    chars, stoi = pickle.load(f)
itos = {i: ch for ch, i in stoi.items()}
vocab_size = len(chars)
print(f"Loaded vocabulary with size {vocab_size}")

print(f"Loading meta from {META_PATH}")
with open(META_PATH, 'rb') as f:
    meta = pickle.load(f)
    
# Configure model based on meta
print("Configuring model...")
config = GPTConfig(
    vocab_size=vocab_size,
    block_size=meta.get('block_size', 64),
    n_layer=meta.get('n_layer', 4),
    n_head=meta.get('n_head', 4),
    n_embd=meta.get('n_embd', 128),
    dropout=meta.get('dropout', 0.1),
    bias=meta.get('bias', True)
)

print("Creating model...")
model = GPT(config)

print(f"Loading model from {MODEL_PATH}")
model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
model.eval()
print("Model loaded successfully")





    # These should match your training config
    config = GPTConfig(vocab_size, 64, 4, 4, 128, 0.1)
    model = GPT(config)

    print(f"Loading model from {MODEL_PATH}")
    # Load on CPU to avoid CUDA memory issues
    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    model.eval()
    print("Model loaded successfully")
    return model, stoi, itos, config
    except Exception as e:
        print(f"Error loading model: {e}")
        raise

print("Initializing model...")
model, stoi, itos, config = load_model()
print("Model initialization complete")

# Rate limiting
RATE_LIMIT = 5  # requests per second
RATE_WINDOW = 1  # seconds
request_counts = defaultdict(list)

def is_rate_limited(ip):
    now = time.time()
    request_counts[ip] = [t for t in request_counts[ip] if t > now - RATE_WINDOW]
    request_counts[ip].append(now)
    return len(request_counts[ip]) > RATE_LIMIT

@app.before_request
def check_rate_limit():
    if request.endpoint == 'chat':
        ip = request.remote_addr
        if is_rate_limited(ip):
            return jsonify({'error': 'Too many requests. Please wait a moment.'}), 429

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        prompt = data.get('prompt', '')
        if not prompt:
            return jsonify({'response': 'No prompt provided.'})

        # Validate prompt length
        if len(prompt) > 5000:
            return jsonify({'error': 'Prompt too long. Maximum length is 5000 characters.'}), 400

        # Encode prompt
        try:
            idx = torch.tensor([[stoi.get(c, 0) for c in prompt]], dtype=torch.long)
        except Exception as e:
            return jsonify({'error': f'Error encoding prompt: {str(e)}'}), 500

        # Generate response
        try:
            with torch.no_grad():
                max_new_tokens = 100
                temperature = 0.8  # Add some randomness to responses
                
                # Initialize generation
                idx = idx[:, -config.block_size:]  # Crop to block_size
                
                for _ in range(max_new_tokens):
                    # Get predictions
                    logits, _ = model(idx)
                    logits = logits[:, -1, :] / temperature
                    probs = torch.nn.functional.softmax(logits, dim=-1)
                    
                    # Sample from the distribution
                    idx_next = torch.multinomial(probs, num_samples=1)
                    idx = torch.cat((idx, idx_next), dim=1)
                    
                    # Stop if we see a newline (you can customize this)
                    if itos[idx_next.item()] == '\n':
                        break

            # Process the response
            try:
                # Convert token indices to text
                generated_sequence = idx[0].tolist()  # Get the first batch item
                full_response = ''.join([itos[i] for i in generated_sequence])
                generated_text = full_response[len(prompt):]  # Extract only the generated portion
                
                # Ensure we have a valid response
                if not generated_text.strip():
                    return jsonify({'error': 'Model generated an empty response. Please try again.'}), 500
                    
                return jsonify({'response': generated_text.strip()})
            except Exception as e:
                return jsonify({'error': 'Error processing model output. Please try again.'}), 500
                
        except Exception as e:
            return jsonify({'error': f'Error generating response: {str(e)}'}), 500

    except Exception as e:
        return jsonify({'error': f'Server error: {str(e)}'}), 500

@app.route('/', methods=['GET'])
def index():
    return 'Void AI backend is running. Use the /chat endpoint for POST requests.', 200

def cleanup_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    import gc
    gc.collect()

@app.after_request
def after_request(response):
    cleanup_memory()
    return response

@app.after_request
def add_security_headers(response):
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    response.headers['Content-Security-Policy'] = "default-src 'self'; img-src 'self' data:; style-src 'self' 'unsafe-inline'"
    return response

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 10000)))
